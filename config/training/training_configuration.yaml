# --- Training operative configuration ---
device: "cuda:0" # Cuda device, i.e. 0 or 0,1,2,3 or cpu
workers: 8 # Maximum number of dataloader workers
batch_size: 8 # Total batch size for all GPUs
nominal_batch_size: 64
img_size: # [train, test] image sizes
  - 640   # train
  - 640   # test
epochs: 100 # Number of training epochs

adam: False # use torch.optim.Adam() optimizer (otherwise SGD)
linear_lr: False # linear learning rate (decay)

resume: False # Resume most recent training. True = latest, can be string to a specific checkpoint
nosave: False # Only save final checkpoint
notest: False # Only test final epoch
logger: "INFO"

# --- Configuration files and paths ---
data: "config/datasets/mnist.yaml" # data.yaml path
architecture_config: "config/architectures/importance_weighted_cnn.yaml"
weights: "" # Initial weights path
hyperparameters: "config/training/hypeparameter_configuration.yaml"  # Hyperparameters path
project: "dumps/train" # save to project/name
name: "efc" # save to project/name
exist_ok: False # existing project/name ok, do not increment

# TODO: ---
# label_smoothing: 0.0 # Label smoothing epsilon
# cache_images: False # Cache images for faster training
# image_weights: False # Use weighted image selection for training
# quad: False # [experimental] may allow some benefits of higher --img size training at lower --img sizes.
# --- Special training parameters ---
#evolve: True # Evolve hyperparameters
#evolve_epochs: 300 # Epochs of evolution